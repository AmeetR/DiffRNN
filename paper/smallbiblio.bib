
@INPROCEEDINGS{Bergstra2013-lr,
title = "Hyperopt: A Python Library for Optimizing the Hyperparameters of Machine Learning Algorithms",
booktitle = "Proceedings of the 12th Python in Science Conference",
author = "Bergstra, James and Yamins, Dan and Cox, David D",
editor = "der Walt, St\'{e}fan van and Millman, Jarrod and Huff, Katy",
pages = "13--20",
year =  2013
}

@ARTICLE{Stanley2002-ug,
title = "Evolving neural networks through augmenting topologies",
author = "Stanley, Kenneth O and Miikkulainen, Risto",
affiliation = "Department of Computer Sciences, The University of Texas at Austin, Austin, TX 78712, USA. kstanley@cs.utexas.edu",
abstract = "An important question in neuroevolution is how to gain an advantage from evolving neural network topologies along with weights. We present a method, NeuroEvolution of Augmenting Topologies (NEAT), which outperforms the best fixed-topology method on a challenging benchmark reinforcement learning task. We claim that the increased efficiency is due to (1) employing a principled method of crossover of different topologies, (2) protecting structural innovation using speciation, and (3) incrementally growing from minimal structure. We test this claim through a series of ablation studies that demonstrate that each component is necessary to the system as a whole and to each other. What results is significantly faster learning. NEAT is also an important contribution to GAs because it shows how it is possible for evolution to both optimize and complexify solutions simultaneously, offering the possibility of evolving increasingly complex solutions over generations, and strengthening the analogy with biological evolution.",
journal = "Evol. Comput.",
volume =  10,
number =  2,
pages = "99--127",
year =  2002,
language = "en"
}

@ARTICLE{He2015-gk,
title = "Deep Residual Learning for Image Recognition",
author = "He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian",
abstract = "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",
month =  "10~" # dec,
year =  2015,
archivePrefix = "arXiv",
primaryClass = "cs.CV",
eprint = "1512.03385"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Yamins2014-us,
title = "Performance-optimized hierarchical models predict neural responses in higher visual cortex",
author = "Yamins, Daniel L K and Hong, Ha and Cadieu, Charles F and Solomon, Ethan A and Seibert, Darren and DiCarlo, James J",
abstract = "The ventral visual stream underlies key human visual object recognition abilities. However, neural encoding in the higher areas of the ventral stream remains poorly understood. Here, we describe a modeling approach that yields a quantitatively accurate model of inferior temporal (IT) cortex, the highest ventral cortical area. Using high-throughput computational techniques, we discovered that, within a class of biologically plausible hierarchical neural network models, there is a strong correlation between a model’s categorization performance and its ability to predict individual IT neural unit response data. To pursue this idea, we then identified a high-performing neural network that matches human performance on a range of recognition tasks. Critically, even though we did not constrain this model to match neural data, its top output layer turns out to be highly predictive of IT spiking responses to complex naturalistic images at both the single site and population levels. Moreover, the model’s intermediate layers are highly predictive of neural responses in the V4 cortex, a midlevel visual area that provides the dominant cortical input to IT. These results show that performance optimization---applied in a biologically appropriate model class---can be used to build quantitative predictive models of neural processing.",
journal = "Proc. Natl. Acad. Sci. U. S. A.",
volume =  111,
number =  23,
pages = "8619--8624",
month =  "10~" # jun,
year =  2014,
language = "en"
}

@ARTICLE{Olshausen1996-vz,
title = "Emergence of simple-cell receptive field properties by learning a sparse code for natural images",
author = "Olshausen, Bruno A and Field, David J",
journal = "Nature",
volume =  381,
number =  6583,
pages = "607--609",
month =  "13~" # jun,
year =  1996,
language = "en"
}
