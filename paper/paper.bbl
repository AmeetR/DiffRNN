% $ biblatex auxiliary file $
% $ biblatex version 2.5 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated as
% required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup

\entry{He2015-gk}{article}{}
  \name{author}{4}{}{%
    {{}%
     {He}{H.}%
     {Kaiming}{K.}%
     {}{}%
     {}{}}%
    {{}%
     {Zhang}{Z.}%
     {Xiangyu}{X.}%
     {}{}%
     {}{}}%
    {{}%
     {Ren}{R.}%
     {Shaoqing}{S.}%
     {}{}%
     {}{}}%
    {{}%
     {Sun}{S.}%
     {Jian}{J.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{HK+1}
  \strng{fullhash}{HKZXRSSJ1}
  \field{sortinit}{H}
  \field{abstract}{%
  Deeper neural networks are more difficult to train. We present a residual
  learning framework to ease the training of networks that are substantially
  deeper than those used previously. We explicitly reformulate the layers as
  learning residual functions with reference to the layer inputs, instead of
  learning unreferenced functions. We provide comprehensive empirical evidence
  showing that these residual networks are easier to optimize, and can gain
  accuracy from considerably increased depth. On the ImageNet dataset we
  evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG
  nets but still having lower complexity. An ensemble of these residual nets
  achieves 3.57\% error on the ImageNet test set. This result won the 1st place
  on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10
  with 100 and 1000 layers. The depth of representations is of central
  importance for many visual recognition tasks. Solely due to our extremely
  deep representations, we obtain a 28\% relative improvement on the COCO
  object detection dataset. Deep residual nets are foundations of our
  submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st
  places on the tasks of ImageNet detection, ImageNet localization, COCO
  detection, and COCO segmentation.%
  }
  \verb{eprint}
  \verb 1512.03385
  \endverb
  \field{title}{Deep Residual Learning for Image Recognition}
  \field{eprinttype}{arXiv}
  \field{eprintclass}{cs.CV}
  \field{year}{2015}
  \warn{\item Invalid format of field 'month'}
\endentry

\entry{Olshausen1996-vz}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Olshausen}{O.}%
     {Bruno~A}{B.~A.}%
     {}{}%
     {}{}}%
    {{}%
     {Field}{F.}%
     {David~J}{D.~J.}%
     {}{}%
     {}{}}%
  }
  \list{language}{1}{%
    {en}%
  }
  \strng{namehash}{OBAFDJ1}
  \strng{fullhash}{OBAFDJ1}
  \field{sortinit}{O}
  \field{number}{6583}
  \field{pages}{607\bibrangedash 609}
  \field{title}{Emergence of simple-cell receptive field properties by learning
  a sparse code for natural images}
  \field{volume}{381}
  \field{journaltitle}{Nature}
  \field{year}{1996}
  \warn{\item Invalid format of field 'month'}
\endentry

\entry{Stanley2002-ug}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Stanley}{S.}%
     {Kenneth~O}{K.~O.}%
     {}{}%
     {}{}}%
    {{}%
     {Miikkulainen}{M.}%
     {Risto}{R.}%
     {}{}%
     {}{}}%
  }
  \list{language}{1}{%
    {en}%
  }
  \strng{namehash}{SKOMR1}
  \strng{fullhash}{SKOMR1}
  \field{sortinit}{S}
  \field{abstract}{%
  An important question in neuroevolution is how to gain an advantage from
  evolving neural network topologies along with weights. We present a method,
  NeuroEvolution of Augmenting Topologies (NEAT), which outperforms the best
  fixed-topology method on a challenging benchmark reinforcement learning task.
  We claim that the increased efficiency is due to (1) employing a principled
  method of crossover of different topologies, (2) protecting structural
  innovation using speciation, and (3) incrementally growing from minimal
  structure. We test this claim through a series of ablation studies that
  demonstrate that each component is necessary to the system as a whole and to
  each other. What results is significantly faster learning. NEAT is also an
  important contribution to GAs because it shows how it is possible for
  evolution to both optimize and complexify solutions simultaneously, offering
  the possibility of evolving increasingly complex solutions over generations,
  and strengthening the analogy with biological evolution.%
  }
  \field{number}{2}
  \field{pages}{99\bibrangedash 127}
  \field{title}{Evolving neural networks through augmenting topologies}
  \field{volume}{10}
  \field{journaltitle}{Evol. Comput.}
  \field{year}{2002}
\endentry

\entry{Yamins2014-us}{article}{}
  \name{author}{6}{}{%
    {{}%
     {Yamins}{Y.}%
     {Daniel L~K}{D.~L.~K.}%
     {}{}%
     {}{}}%
    {{}%
     {Hong}{H.}%
     {Ha}{H.}%
     {}{}%
     {}{}}%
    {{}%
     {Cadieu}{C.}%
     {Charles~F}{C.~F.}%
     {}{}%
     {}{}}%
    {{}%
     {Solomon}{S.}%
     {Ethan~A}{E.~A.}%
     {}{}%
     {}{}}%
    {{}%
     {Seibert}{S.}%
     {Darren}{D.}%
     {}{}%
     {}{}}%
    {{}%
     {DiCarlo}{D.}%
     {James~J}{J.~J.}%
     {}{}%
     {}{}}%
  }
  \list{language}{1}{%
    {en}%
  }
  \strng{namehash}{YDLK+1}
  \strng{fullhash}{YDLKHHCCFSEASDDJJ1}
  \field{sortinit}{Y}
  \field{abstract}{%
  The ventral visual stream underlies key human visual object recognition
  abilities. However, neural encoding in the higher areas of the ventral stream
  remains poorly understood. Here, we describe a modeling approach that yields
  a quantitatively accurate model of inferior temporal (IT) cortex, the highest
  ventral cortical area. Using high-throughput computational techniques, we
  discovered that, within a class of biologically plausible hierarchical neural
  network models, there is a strong correlation between a model’s
  categorization performance and its ability to predict individual IT neural
  unit response data. To pursue this idea, we then identified a high-performing
  neural network that matches human performance on a range of recognition
  tasks. Critically, even though we did not constrain this model to match
  neural data, its top output layer turns out to be highly predictive of IT
  spiking responses to complex naturalistic images at both the single site and
  population levels. Moreover, the model’s intermediate layers are highly
  predictive of neural responses in the V4 cortex, a midlevel visual area that
  provides the dominant cortical input to IT. These results show that
  performance optimization---applied in a biologically appropriate model
  class---can be used to build quantitative predictive models of neural
  processing.%
  }
  \field{number}{23}
  \field{pages}{8619\bibrangedash 8624}
  \field{title}{Performance-optimized hierarchical models predict neural
  responses in higher visual cortex}
  \field{volume}{111}
  \field{journaltitle}{Proc. Natl. Acad. Sci. U. S. A.}
  \field{year}{2014}
  \warn{\item Invalid format of field 'month'}
\endentry

\lossort
\endlossort

\endinput
